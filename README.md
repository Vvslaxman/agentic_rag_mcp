# ğŸ¤– Agentic RAG MCP Bot

A modular, agent-based **Retrieval-Augmented Generation (RAG)** chatbot that supports multi-format document question answering using a custom **Model Context Protocol (MCP)** for structured agent communication.

---

## ğŸš€ Overview

This project demonstrates an end-to-end AI assistant that:
- Accepts documents in diverse formats (PDF, PPTX, CSV, DOCX, TXT, MD)
- Uses semantic search over document embeddings
- Generates answers to user queries using context-aware prompting
- Implements **agent-based orchestration** (Ingestion, Retrieval, LLMResponse, Coordinator)
- Communicates between agents using an in-memory **Model Context Protocol (MCP)**

---

## âœ… Features

- ğŸ“„ Upload multiple document formats
- ğŸ§  SentenceTransformer embeddings (`all-MiniLM-L6-v2`)
- ğŸ” FAISS vector store for semantic retrieval
- ğŸ—‚ï¸ Modular Python agent design
- ğŸ” Traceable message passing between agents using `trace_id`
- ğŸŒ Streamlit frontend for uploads, chat, and results

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/agentic_rag_mcp_bot.git
cd agentic_rag_mcp_bot
pip install -r requirements.txt

```
## ğŸ§ª Usage
``` bash
streamlit run ui/app.py
```

Once running, you can:

- Upload documents (PDF, PPTX, CSV, DOCX, TXT, MD)

- Ask multi-turn questions

- View answers along with source context

## ğŸ—‚ï¸ Project Structure

```agentic_rag_mcp_bot/
â”œâ”€â”€ agents/              # Core agent implementations
â”‚   â”œâ”€â”€ ingestion_agent.py
â”‚   â”œâ”€â”€ retrieval_agent.py
â”‚   â”œâ”€â”€ llm_response_agent.py
â”‚   â””â”€â”€ coordinator_agent.py
â”œâ”€â”€ utils/               # Helpers for parsing, chunking, embedding
â”‚   â”œâ”€â”€ parser_utils.py
â”‚   â”œâ”€â”€ chunking.py
â”‚   â””â”€â”€ embedding_utils.py
â”œâ”€â”€ vector_store/        # FAISS wrapper
â”‚   â””â”€â”€ faiss_store.py
â”œâ”€â”€ mcp/                 # Model Context Protocol definitions
â”‚   â””â”€â”€ mcp_protocol.py
â”œâ”€â”€ ui/                  # Streamlit user interface
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ static/              # Uploaded document storage
â”œâ”€â”€ main.py              # Optional runner for backend-only mode
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ ppt/
    â””â”€â”€ Agentic_RAG_Chatbot_Presentation.pptx


```

## ğŸ” Architecture (Agentic Flow)

1. **User uploads documents and asks a question**
2. **CoordinatorAgent** orchestrates the flow:
    - â¡ï¸ **IngestionAgent** parses, chunks, and embeds docs into the vector store
    - â¡ï¸ **RetrievalAgent** fetches top-k relevant chunks from FAISS
    - â¡ï¸ **LLMResponseAgent** forms a prompt with context and generates an answer

3. Agents communicate using structured MCP messages like:

```json
{
  "type": "RETRIEVE_REQUEST",
  "sender": "CoordinatorAgent",
  "receiver": "RetrievalAgent",
  "trace_id": "abc-123",
  "payload": { "query": "What are the KPIs?" }
}

## ğŸ§© Tech Stack

| Layer         | Tools & Libraries                                      |
|---------------|--------------------------------------------------------|
| **UI**        | Streamlit                                              |
| **Parsing**   | PyMuPDF, python-docx, pandas, python-pptx, markdown    |
| **Embeddings**| SentenceTransformers (`all-MiniLM-L6-v2`)              |
| **Vector Store** | FAISS                                               |
| **LLM**       | OpenAI GPT / HuggingFace (mock fallback)               |
| **Messaging** | Custom MCP (JSON-based, in-memory protocol)            |

---

## âš ï¸ Challenges

- Parsing noisy PPTX/CSV/table-heavy files
- Designing reusable chunking logic for multiple formats
- Balancing agent modularity vs. traceable execution
- Defining consistent MCP JSON schema for message passing

---

## ğŸ”­ Future Scope

- Implement Redis or Kafka for asynchronous MCP message passing
- Stream LLM responses token-by-token (real-time UI updates)
- Integrate long-term memory and multi-turn dialogue support
- Add image/audio/document agents for broader multimodal input
